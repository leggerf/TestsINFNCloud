{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naturale Language Processing notebook example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to NLP notebook example!\n",
    "\n",
    "The first time you run this notebook execute the following line (you can comment it next time): be sure to have requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from time import time\n",
    "import csv\n",
    "import preprocessing as pp\n",
    "import nlp_utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read from minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U liboidcagent requests xmltodict pandas boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "!eval `oidc-keychain` > /dev/null && oidc-token dodas --time=3600 > /tmp/token\n",
    "with open('/tmp/token') as f:\n",
    "    token = f.readlines()[0].split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "r = requests.post(\"https://minio.cloud.infn.it\",\n",
    "                  data={\n",
    "                      'Action':\n",
    "                      \"AssumeRoleWithWebIdentity\",\n",
    "                      'Version': \"2011-06-15\",\n",
    "                      'WebIdentityToken': token,\n",
    "                      'DurationSeconds': 9000\n",
    "                  },\n",
    "                  verify=True)\n",
    "\n",
    "tree = xmltodict.parse(r.content)\n",
    "\n",
    "credentials = dict(tree['AssumeRoleWithWebIdentityResponse']\n",
    "                    ['AssumeRoleWithWebIdentityResult']['Credentials'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3',endpoint_url=\"https://minio.cloud.infn.it\",aws_access_key_id=credentials[\"AccessKeyId\"],\n",
    "                  aws_secret_access_key=credentials[\"SecretAccessKey\"],\n",
    "                  aws_session_token=credentials[\"SessionToken\"],\n",
    "                  config=boto3.session.Config(signature_version='s3v4'),\n",
    "                 verify=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mGH3541\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGH3573\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \"\"\"\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_console_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0mmax_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.max_columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0mnb_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border)\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mSet\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m             \u001b[0mclasses\u001b[0m \u001b[0mto\u001b[0m \u001b[0minclude\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mopening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maddition\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m\"dataframe\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mnotebook\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/formats/html.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, formatter, classes, border, table_id, render_links)\u001b[0m\n\u001b[1;32m     54\u001b[0m         self.col_space = {\n\u001b[1;32m     55\u001b[0m             \u001b[0mcolumn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{value}px\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         }\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                  data stato  codice_regione denominazione_regione        lat  \\\n",
       "0  2020-02-24T18:00:00  ITA   13              Abruzzo               42.351222   \n",
       "1  2020-02-24T18:00:00  ITA   17              Basilicata            40.639471   \n",
       "2  2020-02-24T18:00:00  ITA   18              Calabria              38.905976   \n",
       "3  2020-02-24T18:00:00  ITA   15              Campania              40.839566   \n",
       "4  2020-02-24T18:00:00  ITA   8               Emilia-Romagna        44.494367   \n",
       "\n",
       "        long  ricoverati_con_sintomi  terapia_intensiva  totale_ospedalizzati  \\\n",
       "0  13.398438  0                       0                  0                      \n",
       "1  15.805148  0                       0                  0                      \n",
       "2  16.594402  0                       0                  0                      \n",
       "3  14.250850  0                       0                  0                      \n",
       "4  11.341721  10                      2                  12                     \n",
       "\n",
       "   isolamento_domiciliare  ...  deceduti  casi_da_sospetto_diagnostico  \\\n",
       "0  0                       ...  0        NaN                             \n",
       "1  0                       ...  0        NaN                             \n",
       "2  0                       ...  0        NaN                             \n",
       "3  0                       ...  0        NaN                             \n",
       "4  6                       ...  0        NaN                             \n",
       "\n",
       "   casi_da_screening  totale_casi  tamponi  casi_testati  note  \\\n",
       "0 NaN                 0            5       NaN            NaN    \n",
       "1 NaN                 0            0       NaN            NaN    \n",
       "2 NaN                 0            1       NaN            NaN    \n",
       "3 NaN                 0            10      NaN            NaN    \n",
       "4 NaN                 18           148     NaN            NaN    \n",
       "\n",
       "   ingressi_terapia_intensiva  note_test  note_casi  \n",
       "0 NaN                          NaN        NaN        \n",
       "1 NaN                          NaN        NaN        \n",
       "2 NaN                          NaN        NaN        \n",
       "3 NaN                          NaN        NaN        \n",
       "4 NaN                          NaN        NaN        \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read test CSV\n",
    "\n",
    "read_test = s3.get_object(Bucket='scratch', Key='verlato/dpc-covid19-ita-regioni.csv')\n",
    "df_test = pd.read_csv(read_test['Body'],sep=',')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#read_file\n",
    "read_file = s3.get_object(Bucket='legger', Key='NLPInput/message_example.csv')\n",
    "df = pd.read_csv(read_file['Body'],sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('message_example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Messages are cleaned from file paths, urls etc.(this process can be time requiring)\n",
    "\n",
    "Tokenization is done inline and tokens cleaned from a stopword list are given as input to Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_number=df.shape[0] #message number before cleaning\n",
    "print('...cleaning')\n",
    "start_time_preproc= time()\n",
    "df['cleaned_strings'] = pp.clean_messages(df['error_message'])\n",
    "prepr_time=time() - start_time_preproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.drop_duplicates(['cleaned_strings'],inplace=True)\n",
    "after_number=df_cleaned.shape[0] #message number after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of  messages before cleaning:\",before_number)\n",
    "print(\"number of  messages after cleaning:\",after_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=nlp_utility.callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pp.MyCorpus(df_cleaned)\n",
    "print('...starting training')\n",
    "start_time_train = time()\n",
    "model = Word2Vec(sentences=corpus,compute_loss=True,size=300,window=7, min_count=1, workers=4, iter=30,callbacks=[c])\n",
    "tot_time=time() - start_time_train\n",
    "model.save('example_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, ax=plt.subplots(figsize=(13,8))\n",
    "ax.scatter(np.arange(0,len(c.loss_vec)),c.loss_vec)\n",
    "ax.set_xlabel('iter')\n",
    "ax.set_ylabel('delta loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('preprocessing time: %.2f seconds'% prepr_time)\n",
    "print('training time: %.2f seconds'%tot_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just few words about MyCorpus object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why didn't we feed word2vec model with a plain Python list of tokens? MyCorpus object is much more memory friendly! A list would reside fully in the memory;with MyCorpus object, instead, **at most one vector** resides in RAM **at a time**. In such a way the corpus can be as large as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*print* just outputs address of the object in memory. To see the constituent vectors, letâ€™s iterate over the corpus and print each document vector (one at a time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Word2Vec\n",
    "Let's play a bit with our model to understand what it is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting model vocabulary and total amount of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the \"most similar words\", using the default \"cosine similarity\" measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['pull','push'])) #sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['pull'],negative=['push']))#difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Getting similarity score for each pair and checking not matching word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('pull', 'push'),  #similar role\n",
    "    ('pull', 'copy'),   #often close\n",
    "    ('pull', 'time'),  #sometimes appearing together\n",
    "    ('pull', 'directory'),    # sometimes appearing together\n",
    "    ('pull', 'not'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, model.wv.similarity(w1, w2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.doesnt_match(['pull','push','mode','not']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['error_message'][df_cleaned['error_message'].astype(str).str.contains('pull',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[(df_cleaned['error_message'].astype(str).str.contains('pull',na=False)) & (df_cleaned['error_message'].astype(str).str.contains('directory',na=False) )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if 'pull' and 'push' never appear together the model understands they have similar role (high similarity score): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[(df_cleaned['error_message'].astype(str).str.contains('pull',na=False)) & (df_cleaned['error_message'].astype(str).str.contains('push',na=False) )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary of known words and their frequency in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2c = dict()\n",
    "for item in model.wv.vocab:\n",
    "    w2c[item] = model.wv.vocab[item].count\n",
    "w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))\n",
    "w2cSortedList = list(w2cSorted.keys())\n",
    "w2cSorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting more into NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Supervised Learning\n",
    "Let's test model ability to associate the right error category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='example_model.model'\n",
    "model=Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_sent=nlp_utility.vectorize_messages(df_cleaned['cleaned_strings'],model,tf_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectors_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test,y_train, y_test = train_test_split(vectors_sent,df_cleaned['error_category'].values,test_size=0.2, random_state = 42)\n",
    "mex_train, mex_test,cat_train, cat_test = train_test_split(df_cleaned['error_message'].values,df_cleaned['error_category'].values,test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5,class_weight='balanced',multi_class='auto',solver='lbfgs',max_iter=170)\n",
    "#print(y_train)\n",
    "logreg = logreg.fit(x_train, y_train)\n",
    "#y_pred = logreg.predict(x_test)\n",
    "#print('accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible warnings if some labels in y_test don't appear in y_pred (i.e some labels are never predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_test) - set(y_pred) #if empty you shouldn't have warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'y_test':y_test,'y_pred':y_pred,'mex':mex_test}\n",
    "df_comparison=pd.DataFrame(data=d)\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-D Visualization of vector space: words and error messages (cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allows interactive cursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title='vector_visualization.png'\n",
    "x_vals, y_vals, labels =nlp_utility.reduce_dimensions(model)\n",
    "figsize=(16, 8)\n",
    "nlp_utility.plot_with_matplotlib(x_vals, y_vals, labels,figsize,title=title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on a point to visualize the annotation associated.\n",
    "To move the box, click on it and drag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title='sentence_visualization.png'\n",
    "\n",
    "df_cleaned['vectors_sent']=vectors_sent.tolist()\n",
    "x_vals_sent, y_vals_sent, labels_sent=nlp_utility.reduce_sent_dimensions(df_cleaned)\n",
    "figsize=(16,8)\n",
    "nlp_utility.plot_with_matplotlib(x_vals_sent, y_vals_sent, labels_sent,figsize,npoints=0,save=True,title=title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
