{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naturale Language Processing notebook example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to NLP notebook example!\n",
    "\n",
    "The first time you run this notebook execute the following line (you can comment it next time): be sure to have requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting editdistance==0.5.3\n",
      "  Downloading editdistance-0.5.3.tar.gz (27 kB)\n",
      "Collecting gensim==3.8.1\n",
      "  Downloading gensim-3.8.1.tar.gz (23.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.4 MB 24 kB/s s eta 0:00:01   |██▌                             | 1.8 MB 7.8 MB/s eta 0:00:03     |█████████████                   | 9.5 MB 7.8 MB/s eta 0:00:02     |██████████████▌                 | 10.6 MB 7.8 MB/s eta 0:00:02     |██████████████████▏             | 13.3 MB 7.8 MB/s eta 0:00:02     |████████████████████████        | 17.5 MB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kneed==0.5.1\n",
      "  Downloading kneed-0.5.1-py2.py3-none-any.whl (9.9 kB)\n",
      "Collecting nltk==3.4.5\n",
      "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 19.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.18.1\n",
      "  Downloading numpy-1.18.1-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.6 MB 3.3 kB/s  eta 0:00:01    |█▍                              | 911 kB 17.8 MB/s eta 0:00:02     |████████████████████████        | 15.4 MB 17.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==1.0.1\n",
      "  Downloading pandas-1.0.1-cp38-cp38-manylinux1_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 30 kB/s s eta 0:00:01     |███▉                            | 1.2 MB 19.3 MB/s eta 0:00:01     |███████                         | 2.2 MB 19.3 MB/s eta 0:00:01     |██████████████████████████████▉ | 9.6 MB 19.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyonmttok==1.16.1\n",
      "  Downloading pyonmttok-1.16.1-cp38-cp38-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 6.8 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==0.22.1\n",
      "  Downloading scikit_learn-0.22.1-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0 MB 60 kB/s s eta 0:00:01     |█                               | 204 kB 23.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib<=3.1.2\n",
      "  Downloading matplotlib-3.1.2-cp38-cp38-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1 MB 1.3 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hdbscan==0.8.24\n",
      "  Downloading hdbscan-0.8.24.tar.gz (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 45 kB/s s eta 0:00:011     |███████████████▊                | 2.1 MB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-rake==1.4.5\n",
      "  Downloading python-rake-1.4.5.tar.gz (11 kB)\n",
      "Collecting pytextrank==2.0.1\n",
      "  Downloading pytextrank-2.0.1-py3-none-any.whl (10 kB)\n",
      "Collecting Jinja2==2.11.1\n",
      "  Downloading Jinja2-2.11.1-py2.py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 231 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting rake-nltk\n",
      "  Downloading rake_nltk-1.0.4.tar.gz (7.6 kB)\n",
      "Collecting spacy\n",
      "  Downloading spacy-2.3.5-cp38-cp38-manylinux2014_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 3.5 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting mplcursors\n",
      "  Downloading mplcursors-0.4.tar.gz (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 3.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.18.1\n",
      "  Downloading scipy-1.6.0-cp38-cp38-manylinux1_x86_64.whl (27.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 27.2 MB 3.3 kB/s  eta 0:00:01    |██▍                             | 2.0 MB 18.2 MB/s eta 0:00:02   | 16.6 MB 18.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from gensim==3.8.1->-r requirements.txt (line 2)) (1.15.0)\n",
      "Collecting smart_open>=1.8.1\n",
      "  Downloading smart_open-4.1.2-py3-none-any.whl (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 20.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "  Downloading pytz-2020.5-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 17.8 MB/s eta 0:00:01     |███████████████████████████▋    | 440 kB 17.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /opt/conda/lib/python3.8/site-packages (from pandas==1.0.1->-r requirements.txt (line 6)) (2.8.1)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 41.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<=3.1.2->-r requirements.txt (line 9)) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<=3.1.2->-r requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib<=3.1.2->-r requirements.txt (line 9)) (0.10.0)\n",
      "Collecting cython>=0.27\n",
      "  Using cached Cython-0.29.21-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.16-py2.py3-none-any.whl (19 kB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 35.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting coverage\n",
      "  Downloading coverage-5.4-cp38-cp38-manylinux2010_x86_64.whl (245 kB)\n",
      "\u001b[K     |████████████████████████████████| 245 kB 37.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/lib/python3.8/site-packages (from Jinja2==2.11.1->-r requirements.txt (line 13)) (1.1.1)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp38-cp38-manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 23.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp38-cp38-manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.1-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.8/site-packages (from spacy->-r requirements.txt (line 16)) (2.24.0)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.5-cp38-cp38-manylinux2014_x86_64.whl (186 kB)\n",
      "\u001b[K     |████████████████████████████████| 186 kB 23.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp38-cp38-manylinux2014_x86_64.whl (20 kB)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.8/site-packages (from spacy->-r requirements.txt (line 16)) (4.49.0)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp38-cp38-manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8 MB 535 bytes/s a 0:00:01\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.5-cp38-cp38-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 22.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy->-r requirements.txt (line 16)) (49.6.0.post20200917)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /opt/conda/lib/python3.8/site-packages (from networkx->pytextrank==2.0.1->-r requirements.txt (line 12)) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 16)) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 16)) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 16)) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 16)) (3.0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building wheels for collected packages: editdistance, gensim, nltk, hdbscan, python-rake, rake-nltk, mplcursors\n",
      "  Building wheel for editdistance (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for editdistance: filename=editdistance-0.5.3-cp38-cp38-linux_x86_64.whl size=250737 sha256=2172907a8e8c2f0a5ee77e0b52232c17d38195cd938a8f5ee07e861911a89766\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/30/3f/31/5290eec2de81af55384e72a98e523cd8d487d06fcf5509f13a\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gensim: filename=gensim-3.8.1-cp38-cp38-linux_x86_64.whl size=26721650 sha256=4cf602f546c7d4eda8642062aac8fceaf38837c105282527afcb451276c196d0\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/33/de/03/7346ae70da7f980f78569668caf78fb2d678b176e549557c7d\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449905 sha256=b0a667e00149068c4e9fb81a30ca4e27044b855bdc676e0a3bd80d18c0c6e272\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/23/18/48/8fd6ec11da38406b309470566d6f099c04805d2ec61d7829e7\n",
      "  Building wheel for hdbscan (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdbscan: filename=hdbscan-0.8.24-cp38-cp38-linux_x86_64.whl size=3835150 sha256=56a7795d9bc8044f14bae4d9638212ef898d7121d4a2957de3a371f352f7df58\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/4e/e1/c6/cea6e82b76e9c48091a8650230849bfadddb01ace0cbd395ef\n",
      "  Building wheel for python-rake (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-rake: filename=python_rake-1.4.5-py3-none-any.whl size=13479 sha256=993537b05cca6ba574427c5c35e5396303a004c4a0a64c38f64f8a4bf31e9d3e\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/44/f5/97/25b1df3899b74c91598ed3f2ac47610a2983f4a5a06b27844f\n",
      "  Building wheel for rake-nltk (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/conda/bin/python3.8 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-7rwueob9/rake-nltk/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-7rwueob9/rake-nltk/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-igmon9hd\n",
      "       cwd: /tmp/pip-install-7rwueob9/rake-nltk/\n",
      "  Complete output (57 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib\n",
      "  creating build/lib/rake_nltk\n",
      "  copying rake_nltk/rake.py -> build/lib/rake_nltk\n",
      "  copying rake_nltk/__init__.py -> build/lib/rake_nltk\n",
      "  copying rake_nltk/__version__.py -> build/lib/rake_nltk\n",
      "  installing to build/bdist.linux-x86_64/wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating build/bdist.linux-x86_64\n",
      "  creating build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/rake_nltk\n",
      "  copying build/lib/rake_nltk/rake.py -> build/bdist.linux-x86_64/wheel/rake_nltk\n",
      "  copying build/lib/rake_nltk/__init__.py -> build/bdist.linux-x86_64/wheel/rake_nltk\n",
      "  copying build/lib/rake_nltk/__version__.py -> build/bdist.linux-x86_64/wheel/rake_nltk\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  writing rake_nltk.egg-info/PKG-INFO\n",
      "  writing dependency_links to rake_nltk.egg-info/dependency_links.txt\n",
      "  writing requirements to rake_nltk.egg-info/requires.txt\n",
      "  writing top-level names to rake_nltk.egg-info/top_level.txt\n",
      "  reading manifest file 'rake_nltk.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  writing manifest file 'rake_nltk.egg-info/SOURCES.txt'\n",
      "  Copying rake_nltk.egg-info to build/bdist.linux-x86_64/wheel/rake_nltk-1.0.4-py3.8.egg-info\n",
      "  running install_scripts\n",
      "  Running post installation tasks\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/tmp/pip-install-7rwueob9/rake-nltk/setup.py\", line 44, in <module>\n",
      "      setup(\n",
      "    File \"/opt/conda/lib/python3.8/site-packages/setuptools/__init__.py\", line 163, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"/opt/conda/lib/python3.8/distutils/core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"/opt/conda/lib/python3.8/distutils/dist.py\", line 966, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"/opt/conda/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/opt/conda/lib/python3.8/site-packages/wheel/bdist_wheel.py\", line 326, in run\n",
      "      self.run_command('install')\n",
      "    File \"/opt/conda/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/opt/conda/lib/python3.8/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/tmp/pip-install-7rwueob9/rake-nltk/setup.py\", line 36, in run\n",
      "      self.execute(_post_install, [], msg=\"Running post installation tasks\")\n",
      "    File \"/opt/conda/lib/python3.8/distutils/cmd.py\", line 335, in execute\n",
      "      util.execute(func, args, msg, dry_run=self.dry_run)\n",
      "    File \"/opt/conda/lib/python3.8/distutils/util.py\", line 303, in execute\n",
      "      func(*args)\n",
      "    File \"/tmp/pip-install-7rwueob9/rake-nltk/setup.py\", line 17, in _post_install\n",
      "      import nltk\n",
      "  ModuleNotFoundError: No module named 'nltk'\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for rake-nltk\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for rake-nltk\n",
      "  Building wheel for mplcursors (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mplcursors: filename=mplcursors-0.4-py3-none-any.whl size=19968 sha256=9e534ed7ee18171a71be6b3f758fc2d342742934c9a6c2ca0e6f43a335bd33a0\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/39/51/00/b726dab526916d297e4416c77aa031f4fdb9747e110593537d\n",
      "Successfully built editdistance gensim nltk hdbscan python-rake mplcursors\n",
      "Failed to build rake-nltk\n",
      "Installing collected packages: editdistance, numpy, scipy, smart-open, gensim, matplotlib, joblib, scikit-learn, kneed, nltk, pytz, pandas, pyonmttok, cython, hdbscan, python-rake, murmurhash, cymem, preshed, plac, wasabi, srsly, blis, catalogue, thinc, spacy, graphviz, networkx, coverage, pytextrank, Jinja2, google-pasta, rake-nltk, mplcursors\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.4\n",
      "    Uninstalling numpy-1.19.4:\n",
      "      Successfully uninstalled numpy-1.19.4\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.3.3\n",
      "    Uninstalling matplotlib-3.3.3:\n",
      "      Successfully uninstalled matplotlib-3.3.3\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 2.11.2\n",
      "    Uninstalling Jinja2-2.11.2:\n",
      "      Successfully uninstalled Jinja2-2.11.2\n",
      "    Running setup.py install for rake-nltk ... \u001b[?25ldone\n",
      "\u001b[33m  DEPRECATION: rake-nltk was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n",
      "\u001b[?25hSuccessfully installed Jinja2-2.11.1 blis-0.7.4 catalogue-1.0.0 coverage-5.4 cymem-2.0.5 cython-0.29.21 editdistance-0.5.3 gensim-3.8.1 google-pasta-0.2.0 graphviz-0.16 hdbscan-0.8.24 joblib-1.0.0 kneed-0.5.1 matplotlib-3.1.2 mplcursors-0.4 murmurhash-1.0.5 networkx-2.5 nltk-3.4.5 numpy-1.18.1 pandas-1.0.1 plac-1.1.3 preshed-3.0.5 pyonmttok-1.16.1 pytextrank-2.0.1 python-rake-1.4.5 pytz-2020.5 rake-nltk-1.0.4 scikit-learn-0.22.1 scipy-1.6.0 smart-open-4.1.2 spacy-2.3.5 srsly-1.0.5 thinc-7.4.5 wasabi-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from time import time\n",
    "import csv\n",
    "import preprocessing as pp\n",
    "import nlp_utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read from minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting liboidcagent\n",
      "  Downloading liboidcagent-0.3.0-py3-none-any.whl (5.1 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 29 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting xmltodict\n",
      "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.2.1-cp38-cp38-manylinux1_x86_64.whl (9.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.7 MB 8.7 kB/s  eta 0:00:01    |███████████████████▎            | 5.8 MB 4.3 MB/s eta 0:00:01     |██████████████████████████▉     | 8.1 MB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.16.59-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 26.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyNaCl>=1.2.0\n",
      "  Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB)\n",
      "\u001b[K     |████████████████████████████████| 961 kB 19.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.16.5 in /opt/conda/lib/python3.8/site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas) (2020.5)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.20.0,>=1.19.59\n",
      "  Downloading botocore-1.19.59-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 39 kB/s s eta 0:00:01     |███                             | 655 kB 16.3 MB/s eta 0:00:01     |████████████▎                   | 2.8 MB 16.3 MB/s eta 0:00:01     |███████████████▌                | 3.5 MB 16.3 MB/s eta 0:00:01     |██████████████████▋             | 4.2 MB 16.3 MB/s eta 0:00:01     |██████████████████████          | 5.0 MB 16.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.4-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 303 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.8/site-packages (from PyNaCl>=1.2.0->liboidcagent) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from PyNaCl>=1.2.0->liboidcagent) (1.14.3)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.4.1->PyNaCl>=1.2.0->liboidcagent) (2.20)\n",
      "Installing collected packages: PyNaCl, liboidcagent, requests, xmltodict, pandas, jmespath, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.1\n",
      "    Uninstalling pandas-1.0.1:\n",
      "      Successfully uninstalled pandas-1.0.1\n",
      "Successfully installed PyNaCl-1.4.0 boto3-1.16.59 botocore-1.19.59 jmespath-0.10.0 liboidcagent-0.3.0 pandas-1.2.1 requests-2.25.1 s3transfer-0.3.4 xmltodict-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U liboidcagent requests xmltodict pandas boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!eval `oidc-keychain` > /dev/null && oidc-token dodas --time=3600 > /tmp/token\n",
    "with open('/tmp/token') as f:\n",
    "    token = f.readlines()[0].split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "r = requests.post(\"https://minio.cloud.infn.it\",\n",
    "                  data={\n",
    "                      'Action':\n",
    "                      \"AssumeRoleWithWebIdentity\",\n",
    "                      'Version': \"2011-06-15\",\n",
    "                      'WebIdentityToken': token,\n",
    "                      'DurationSeconds': 9000\n",
    "                  },\n",
    "                  verify=True)\n",
    "\n",
    "tree = xmltodict.parse(r.content)\n",
    "\n",
    "credentials = dict(tree['AssumeRoleWithWebIdentityResponse']\n",
    "                    ['AssumeRoleWithWebIdentityResult']['Credentials'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3',endpoint_url=\"https://minio.cloud.infn.it\",aws_access_key_id=credentials[\"AccessKeyId\"],\n",
    "                  aws_secret_access_key=credentials[\"SecretAccessKey\"],\n",
    "                  aws_session_token=credentials[\"SessionToken\"],\n",
    "                  config=boto3.session.Config(signature_version='s3v4'),\n",
    "                 verify=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mGH3541\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGH3573\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \"\"\"\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_console_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0mmax_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"display.max_columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0mnb_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border)\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mSet\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m             \u001b[0mclasses\u001b[0m \u001b[0mto\u001b[0m \u001b[0minclude\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mopening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maddition\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m\"dataframe\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mnotebook\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/formats/html.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, formatter, classes, border, table_id, render_links)\u001b[0m\n\u001b[1;32m     54\u001b[0m         self.col_space = {\n\u001b[1;32m     55\u001b[0m             \u001b[0mcolumn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{value}px\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         }\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                  data stato  codice_regione denominazione_regione        lat  \\\n",
       "0  2020-02-24T18:00:00   ITA              13               Abruzzo  42.351222   \n",
       "1  2020-02-24T18:00:00   ITA              17            Basilicata  40.639471   \n",
       "2  2020-02-24T18:00:00   ITA              18              Calabria  38.905976   \n",
       "3  2020-02-24T18:00:00   ITA              15              Campania  40.839566   \n",
       "4  2020-02-24T18:00:00   ITA               8        Emilia-Romagna  44.494367   \n",
       "\n",
       "        long  ricoverati_con_sintomi  terapia_intensiva  totale_ospedalizzati  \\\n",
       "0  13.398438                       0                  0                     0   \n",
       "1  15.805148                       0                  0                     0   \n",
       "2  16.594402                       0                  0                     0   \n",
       "3  14.250850                       0                  0                     0   \n",
       "4  11.341721                      10                  2                    12   \n",
       "\n",
       "   isolamento_domiciliare  ...  deceduti  casi_da_sospetto_diagnostico  \\\n",
       "0                       0  ...         0                           NaN   \n",
       "1                       0  ...         0                           NaN   \n",
       "2                       0  ...         0                           NaN   \n",
       "3                       0  ...         0                           NaN   \n",
       "4                       6  ...         0                           NaN   \n",
       "\n",
       "   casi_da_screening  totale_casi  tamponi  casi_testati  note  \\\n",
       "0                NaN            0        5           NaN   NaN   \n",
       "1                NaN            0        0           NaN   NaN   \n",
       "2                NaN            0        1           NaN   NaN   \n",
       "3                NaN            0       10           NaN   NaN   \n",
       "4                NaN           18      148           NaN   NaN   \n",
       "\n",
       "   ingressi_terapia_intensiva  note_test  note_casi  \n",
       "0                         NaN        NaN        NaN  \n",
       "1                         NaN        NaN        NaN  \n",
       "2                         NaN        NaN        NaN  \n",
       "3                         NaN        NaN        NaN  \n",
       "4                         NaN        NaN        NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read test CSV\n",
    "\n",
    "read_test = s3.get_object(Bucket='scratch', Key='verlato/dpc-covid19-ita-regioni.csv')\n",
    "df_test = pd.read_csv(read_test['Body'],sep=',')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#read_file\n",
    "read_file = s3.get_object(Bucket='legger', Key='NLPInput/message_example.csv')\n",
    "df = pd.read_csv(read_file['Body'],sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('message_example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Messages are cleaned from file paths, urls etc.(this process can be time requiring)\n",
    "\n",
    "Tokenization is done inline and tokens cleaned from a stopword list are given as input to Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_number=df.shape[0] #message number before cleaning\n",
    "print('...cleaning')\n",
    "start_time_preproc= time()\n",
    "df['cleaned_strings'] = pp.clean_messages(df['error_message'])\n",
    "prepr_time=time() - start_time_preproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.drop_duplicates(['cleaned_strings'],inplace=True)\n",
    "after_number=df_cleaned.shape[0] #message number after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of  messages before cleaning:\",before_number)\n",
    "print(\"number of  messages after cleaning:\",after_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=nlp_utility.callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pp.MyCorpus(df_cleaned)\n",
    "print('...starting training')\n",
    "start_time_train = time()\n",
    "model = Word2Vec(sentences=corpus,compute_loss=True,size=300,window=7, min_count=1, workers=4, iter=30,callbacks=[c])\n",
    "tot_time=time() - start_time_train\n",
    "model.save('example_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, ax=plt.subplots(figsize=(13,8))\n",
    "ax.scatter(np.arange(0,len(c.loss_vec)),c.loss_vec)\n",
    "ax.set_xlabel('iter')\n",
    "ax.set_ylabel('delta loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('preprocessing time: %.2f seconds'% prepr_time)\n",
    "print('training time: %.2f seconds'%tot_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just few words about MyCorpus object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why didn't we feed word2vec model with a plain Python list of tokens? MyCorpus object is much more memory friendly! A list would reside fully in the memory;with MyCorpus object, instead, **at most one vector** resides in RAM **at a time**. In such a way the corpus can be as large as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*print* just outputs address of the object in memory. To see the constituent vectors, let’s iterate over the corpus and print each document vector (one at a time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Word2Vec\n",
    "Let's play a bit with our model to understand what it is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting model vocabulary and total amount of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the \"most similar words\", using the default \"cosine similarity\" measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['pull','push'])) #sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['pull'],negative=['push']))#difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Getting similarity score for each pair and checking not matching word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('pull', 'push'),  #similar role\n",
    "    ('pull', 'copy'),   #often close\n",
    "    ('pull', 'time'),  #sometimes appearing together\n",
    "    ('pull', 'directory'),    # sometimes appearing together\n",
    "    ('pull', 'not'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, model.wv.similarity(w1, w2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.doesnt_match(['pull','push','mode','not']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['error_message'][df_cleaned['error_message'].astype(str).str.contains('pull',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[(df_cleaned['error_message'].astype(str).str.contains('pull',na=False)) & (df_cleaned['error_message'].astype(str).str.contains('directory',na=False) )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if 'pull' and 'push' never appear together the model understands they have similar role (high similarity score): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[(df_cleaned['error_message'].astype(str).str.contains('pull',na=False)) & (df_cleaned['error_message'].astype(str).str.contains('push',na=False) )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary of known words and their frequency in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2c = dict()\n",
    "for item in model.wv.vocab:\n",
    "    w2c[item] = model.wv.vocab[item].count\n",
    "w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))\n",
    "w2cSortedList = list(w2cSorted.keys())\n",
    "w2cSorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting more into NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Supervised Learning\n",
    "Let's test model ability to associate the right error category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='example_model.model'\n",
    "model=Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_sent=nlp_utility.vectorize_messages(df_cleaned['cleaned_strings'],model,tf_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectors_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test,y_train, y_test = train_test_split(vectors_sent,df_cleaned['error_category'].values,test_size=0.2, random_state = 42)\n",
    "mex_train, mex_test,cat_train, cat_test = train_test_split(df_cleaned['error_message'].values,df_cleaned['error_category'].values,test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5,class_weight='balanced',multi_class='auto',solver='lbfgs',max_iter=170)\n",
    "#print(y_train)\n",
    "logreg = logreg.fit(x_train, y_train)\n",
    "#y_pred = logreg.predict(x_test)\n",
    "#print('accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible warnings if some labels in y_test don't appear in y_pred (i.e some labels are never predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_test) - set(y_pred) #if empty you shouldn't have warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'y_test':y_test,'y_pred':y_pred,'mex':mex_test}\n",
    "df_comparison=pd.DataFrame(data=d)\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-D Visualization of vector space: words and error messages (cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allows interactive cursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title='vector_visualization.png'\n",
    "x_vals, y_vals, labels =nlp_utility.reduce_dimensions(model)\n",
    "figsize=(16, 8)\n",
    "nlp_utility.plot_with_matplotlib(x_vals, y_vals, labels,figsize,title=title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on a point to visualize the annotation associated.\n",
    "To move the box, click on it and drag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title='sentence_visualization.png'\n",
    "\n",
    "df_cleaned['vectors_sent']=vectors_sent.tolist()\n",
    "x_vals_sent, y_vals_sent, labels_sent=nlp_utility.reduce_sent_dimensions(df_cleaned)\n",
    "figsize=(16,8)\n",
    "nlp_utility.plot_with_matplotlib(x_vals_sent, y_vals_sent, labels_sent,figsize,npoints=0,save=True,title=title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
