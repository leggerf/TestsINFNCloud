{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMEMBER to start the spark cluster with the little star icon before executing the notebook\n",
    "\n",
    "\n",
    "# Hands-on Day 1\n",
    "\n",
    "- course [slides](https://github.com/leggerf/MLCourse-1920/blob/master/Slides/Day1/ML-day1.pdf)\n",
    "\n",
    "# You'll learn\n",
    "\n",
    "- familiarize with jupyter notebooks, numpy and pandas\n",
    "\n",
    "## Input data\n",
    "- efficient data format: convert CSV to Parquet\n",
    "- create input vector with features for MLLib. Format of the input depends on chosen ML library\n",
    "\n",
    "## Visualization\n",
    "- explore dataset, plot features\n",
    "- correlation matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset description\n",
    "\n",
    "The dataset used in this example is described [here](https://archive.ics.uci.edu/ml/datasets/HIGGS). It is a binary classification problem where the goal is to train a classifier able to distinguish between a signal process, the production of new theoretical Higgs bosons, and a background process with identical decay products but distinct kinematic features.\n",
    "\n",
    "Each row of this dataset contains 28 features plus the label:\n",
    "\n",
    "- 21 low-level features which represent the basic measure made by the particle detector\n",
    " -        Momentum of the observed paricles\n",
    " -        Missing transverse momentum\n",
    " -        Jets and b-tagging information\n",
    "- 7 high-level features computed from the low-level features that encode the knowledge of the different intermediate states of the two processes (reconstructed invariant masses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the execution environment\n",
    "\n",
    "Your code will run on a single dedicated server with 24 cores (hyperthreading enabled) and 192 GB of RAM. \n",
    "All the services needed for this tutorial are deployed as Kubernetes applications on this server. These include:\n",
    "* JupytherHub\n",
    "* Jupyter single-user servers\n",
    "* the HDFS file-system\n",
    "* Spark Clusters on demand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (1.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.8/site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas) (2020.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.3.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.1.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.19.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2020.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Check out these custom functions\n",
    "from custom_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in local csv files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 852 ms, sys: 124 ms, total: 976 ms\n",
      "Wall time: 988 ms\n"
     ]
    }
   ],
   "source": [
    "inputFile = 'Higgs100k.csv'\n",
    "\n",
    "#into panda DF\n",
    "%time dataset = pd.read_csv(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 288 ms, total: 15.6 s\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "#then into spark dataframe...remember to start spark with the little icon\n",
    "\n",
    "%time df = spark.createDataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: None\n"
     ]
    }
   ],
   "source": [
    "#In how many partitions is the dataframe distributed?\n",
    "print(\"Number of partitions: {}\".format(df.rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(df.rdd.partitioner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's have a look at the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time total_events = df.count()\n",
    "\n",
    "print('There are '+str(total_events)+' events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "- Create a function that makes a plot of any of the above variable for signal versus background (using the label variable to discriminate)\n",
    "  - see an example of the plot in the hands-on [slides](https://github.com/leggerf/MLCourse-1920/blob/master/Slides/Day1/ML-handson-day1.pdf)\n",
    "  - the function should take as input the dataframe *df* and the variable name. For example `plotSignalvsBg(df, 'm_bb')`\n",
    "  - to develop the code, use the 100k dataset, so that debugging goes quicker\n",
    "- try to plot a few input variables and try to understand which ones are more promising to distinguish signal from background  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot invariant mass distribution \n",
    "plotSignalvsBg(df, 'm_bb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - Bonus\n",
    "\n",
    "#### Create the input feature vector\n",
    "\n",
    "- Libraries for ML tipically take as inputs data in a very specific format. Documentation on how to do data preprocessing in Spark: https://spark.apache.org/docs/latest/ml-features.html\n",
    "- Try to add to the dataframe df a new column, called 'features' which is a vector column with all the variables above except for 'label'\n",
    "   - features = [lepton_pT, lepton_eta, lepton_phi, ...]\n",
    "   - Hint: look at the VectorAssembler transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input vector for ML (features)\n",
    "feature = df.columns\n",
    "feature.remove('label')\n",
    "\n",
    "#VectorAssembler is a transformer that combines a given list of columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature, outputCol='features')\n",
    "df = assembler.transform(df)\n",
    "\n",
    "#take a look at how there is now a features column\n",
    "display(df)\n",
    "df.select('features').take(1)\n",
    "\n",
    "#note also that features are already standardised between -1 and 1, so rescaling won't be needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print (or draw) the correlation matrix (a table showing correlation coefficients between variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrelation(df, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when you're done, stop spark, this will release the resources you're using\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.driver.maxResultSize",
     "value": "0"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
